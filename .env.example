# LalaSearch Environment Configuration
# Copy this file to .env and update values as needed

# === Agent Configuration ===
# Agent operational mode: worker, manager, or all
# - worker: Only process crawl queue
# - manager: Only serve HTTP API
# - all: Both worker and manager functionality
AGENT_MODE=all

# Deployment mode: single_tenant or multi_tenant
# - single_tenant: Self-hosted open source installation (default)
# - multi_tenant: SaaS/hosted version with one keyspace per customer
DEPLOYMENT_MODE=single_tenant

# === Database Configuration (Apache Cassandra) ===
# Comma-separated list of Cassandra hosts
# For local development: 127.0.0.1:9042
# For Docker: cassandra:9042
CASSANDRA_HOSTS=127.0.0.1:9042

# Cassandra system keyspace - stores the global tenant registry (tenants table)
CASSANDRA_SYSTEM_KEYSPACE=lalasearch_system

# Cassandra tenant keyspace - stores crawling data for this installation
# Single-tenant: one keyspace for all data
# Multi-tenant: one keyspace per customer (lalasearch_<tenant_id>)
CASSANDRA_KEYSPACE=lalasearch_default

# === Search Engine Configuration (Meilisearch) ===
# Meilisearch host address
# For local development: 127.0.0.1:7700
# For Docker: meilisearch:7700
MEILISEARCH_HOST=127.0.0.1:7700

# === Crawler Configuration ===
# Interval in seconds between queue polling cycles
QUEUE_POLL_INTERVAL_SECS=5

# User agent string for web crawler
USER_AGENT=LalaSearchBot/0.1

# === Runtime Configuration ===
# Environment: dev or prod
# - dev: Feature flags default to enabled (crawling_enabled=true, etc.)
# - prod: Feature flags default to disabled for safety
# Database settings always override these defaults
ENVIRONMENT=dev

# Rust log level: trace, debug, info, warn, error
RUST_LOG=info

# === S3 Storage Configuration ===
# S3-compatible storage for crawled HTML content
# Supports AWS S3, SeaweedFS, DigitalOcean Spaces, Wasabi, etc.

# S3 endpoint URL (required)
# For SeaweedFS: http://seaweedfs:8333 or http://127.0.0.1:8333
# For AWS S3: https://s3.amazonaws.com or https://s3.{region}.amazonaws.com
S3_ENDPOINT=http://127.0.0.1:8333

# S3 region (optional for SeaweedFS, required for AWS S3)
S3_REGION=us-east-1

# S3 bucket name for storing crawled content
S3_BUCKET=lalasearch-content

# S3 access credentials (SeaweedFS doesn't require authentication by default)
S3_ACCESS_KEY=any
S3_SECRET_KEY=any

# Enable gzip compression for stored content (true/false)
S3_COMPRESS_CONTENT=true

# Minimum content size in bytes to trigger compression (default: 1024)
S3_COMPRESS_MIN_SIZE=1024

# === Authentication Configuration ===
# SMTP server for sending magic link emails
#
# Option 1: Local Postfix (Docker) - Direct sending, emails may go to spam
# SMTP_HOST=postfix
# SMTP_PORT=25
# SMTP_USERNAME=
# SMTP_PASSWORD=
# SMTP_TLS=false
#
# Option 2: External SMTP service (recommended for production)
SMTP_HOST=smtp.example.com
SMTP_PORT=587
SMTP_USERNAME=your-smtp-username
SMTP_PASSWORD=your-smtp-password
SMTP_TLS=true

# Email sender settings
SMTP_FROM_EMAIL=noreply@yourdomain.com
SMTP_FROM_NAME=LalaSearch

# Application base URL (for magic link URLs in emails)
APP_BASE_URL=http://localhost:3000

# Session duration in days (default: 365)
SESSION_MAX_AGE_DAYS=365

# Magic link expiry in minutes (default: 15)
MAGIC_LINK_EXPIRY_MINUTES=15

# Organization invitation expiry in days (default: 7)
INVITATION_EXPIRY_DAYS=7

# === Build Configuration (Optional) ===
# Override patch version for CI/CD builds
# LALA_PATCH_VERSION=123
